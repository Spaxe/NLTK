{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<img style=\"float:left\" src=\"http://ipython.org/_static/IPy_header.png\" />\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1: Orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Welcome to the *IPython Notebook*. Through this interface, you'll be learning a lot of things:\n",
    "\n",
    "* A Programming language: **Python**\n",
    "* A Python library: **NLTK**\n",
    "* Overlapping research areas: **Corpus linguistics**, **Natural language processing**, **Distant reading**\n",
    "* Additional skills: **Regular Expressions**, some **Shell commands**, and **tips on managing your data**\n",
    "\n",
    "You can head [here](https://github.com/resbaz/lessons/blob/master/nltk/README.md) for the fully articulated overview of the course, but we'll almost always stay within IPython. \n",
    "Remember, everything we cover here will remain available to you after ResBaz is over, including these Notebooks. It's all accessible at the [ResBaz GitHub](https://github.com/resbaz/lessons/tree/master/nltk).\n",
    "\n",
    "**Any questions before we begin?**\n",
    "\n",
    "Alright, we're off!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programming languages like Python are great for processing data. In order to apply it to *text*, we need to think about our text as data.\n",
    "This means being aware of how text is structured, what extra information might be encoded in it, and how to manage to give the best results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Natural Language Toolkit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We'll be covering some of the theory behind corpus linguistics later on, but let's start by looking at some of the tasks NLTK can help you with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK is a Python Library for working with written language data. It is free and extensively documented. Many areas we'll be covering are treated in more detail in the NLTK Book, available free online from [here](http://www.nltk.org/book/).\n",
    "\n",
    "> Note: NLTK provides tools for tasks ranging from very simple (counting words in a text) to very complex (writing and training parsers, etc.). Many advanced tasks are beyond the scope of this course, but by the time we're done, you should understand Python and NLTK well enough to perform these tasks on your own!\n",
    "\n",
    "We will start by importing NLTK, setting a path to NLTK resources, and downloading some additional stuff.\n",
    "\n",
    "> Note: If you are not familiar with Python, don't sweat! We will walk through the basics throughout this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division # __future__ ensures Python2 and Python3 interopability\n",
    "from IPython.display import display, clear_output # clear output from download\n",
    "import nltk # import Natural Language Processing Toolkit library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, we've got to import some corpora used in the book as well...\n",
    "\n",
    "> Note: this location is specific to virtual machines used with the Dit4c platform. If running elswhere, try running nltk.download(...) with your own path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_nltk_dir = \"/home/researcher/nltk_data\" # specify our data directory\n",
    "if user_nltk_dir not in nltk.data.path: # make sure nltk can access this directory\n",
    "    nltk.data.path.insert(0, user_nltk_dir)\n",
    "nltk.download(\"book\", download_dir=user_nltk_dir) # download sample materials to data directory\n",
    "clear_output() # clear the large amount of text we just generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.book import *  \n",
    "# asterisk means 'everything'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the book has assigned variable names to ten corpora. We can call these names easily: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Vocabulary: lexical richness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK makes it really easy to get basic information about the size of a text and the complexity of its vocabulary.\n",
    "\n",
    "* **`len()`** gives the number of symbols or 'tokens' in your text. This is the total number of words and items of punctuation.\n",
    "\n",
    "* **`set()`** gives you a list of all the tokens in the text, without the duplicates. Hence, **`len(set(text3))`** will give you the total number unique tokens. Remember this still includes punctuation. \n",
    "\n",
    "* **`sorted()`** places items in the list into alphabetical order, with punctuation symbols and capitalised words first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(text3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(set(text3)) [0:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate *lexical richness* of a text by dividing the total number of words by the number of unique words. This tells us how diverse and varied the vocabulary is, which could be used to infer the author's writing.\n",
    "\n",
    "We can also count the number of times a word is used and calculate what percentage of the text it represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(text3)/len(set(text3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text4.count(\"American\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge!** \n",
    "\n",
    "How would you calculate the percentage of Text 4 that is taken up by the word \"America\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "100.0*text4.count(\"America\")/len(text4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Vocabulary: concordances, similar contexts, and dispersion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`concordance()`** shows you a word in context and is useful if you want to be able to discuss the ways in which a word is used in a text. \n",
    "\n",
    "* **`similar()`** will find words used in similar contexts; remember it is not looking for synonyms, although the results may include synonyms.\n",
    "\n",
    "* **`common_contexts()`** looks for adjacent words commonly used for both words, which says how similar the author uses these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text1.concordance(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text1.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text2.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text2.common_contexts([\"monstrous\", \"very\"])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`collocations()`**: we can also find words that typically occur together, which tend to be very specific to a text or genre of texts. We'll talk more about these features and how to use them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text4.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python also lets you create graphs to display data. To represent information about a text graphically, import the Python library **`matplotlib`**. We can then generate a dispersion plot that shows where given words occur in a text.\n",
    "\n",
    "**`%matplotlib inline`** is an IPython-Notebook-specific command that tells it to display the plot below the code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from nltk.draw.dispersion import dispersion_plot\n",
    "dispersion_plot(text1, [\"whale\", \"storm\", \"calm\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge!**\n",
    "\n",
    "Create a dispersion plot for the terms \"citizens\", \"democracy\", \"freedom\", \"duties\" and \"America\" in the innaugural address corpus.\n",
    "What do you think it tells you? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dispersion_plot(text4, [\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"]) # plot five words longitudinally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPython Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we've been writing Python code in an IPython notebook. Why?\n",
    "\n",
    "1. The main strength of IPython is that you can run bits of code individually, so you don't have to keep repeating things. For example, if you scroll up to the last function and replace the 50 with 2, you can re-run that code and get the new answer. \n",
    "2. IPython allows you to display images alongside code, and to save the input and output together.\n",
    "3. IPython makes learning a bit easier, as mistakes are easier to find and do not break an entire workflow.\n",
    "\n",
    "You can get more information on IPython, including how to install it on your own machine, at the [IPython Homepage](http://ipython.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Defining a variable\n",
    "\n",
    "In Python, we give the items we're working with names, a process called assignment. For instance, in the NLTK corpus, 'Sense and Sensibility' has been assigned the name 'text2', which is much easier to work with. \n",
    "We also assigend the name 'sent' to the sentence that we created in the previous exercise, so that we could then instruct Python to do various things with it. Assigning a variable in python looks like this: \n",
    "\n",
    "`variable = expression   `\n",
    "\n",
    "You can call your variables (almost) anything you like, but it's a good idea to pick names that will be meaningful and easy to type. You can't use words that already have a meaning in Python, such as import, def, or not. If you try to use a word that is reserved, you'll get a syntax error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string = 'user'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significant Whitespace\n",
    "One thing that makes Python unique is that whitespace at the start of the line (use four spaces for consistency!) is meaningful. \n",
    "In many other languages, whitespace at the start of lines is simply a readability convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fix this whitespace problem!\n",
    "if string == 'user':\n",
    "    print('Phew, fixed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, whitespace tells both Python and human readers where things start and stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll talk about *functions*. Advantages of functions are:\n",
    "\n",
    "1. Save you typing\n",
    "2. You can be sure you're doing exactly the same operation every time\n",
    "\n",
    "Functions can include a **`return`** statement which gives an output to assign to variables or other functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def welcome(name):\n",
    "    return 'Welcome, {}!'.format(name) # .format() lets you include variables inside strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`def`** is how we *define* a function in Python.\n",
    "\n",
    "Notice that it doesn't do anything by itself. It needs to actually be *called*, and given some data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "welcome('Kim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may wish to repeat an operation multiple times looking at different texts or different terms within a text. Instead of re-entering the formula every time, you can assign a name and set of actions to a particular task.\n",
    "\n",
    "> **Note**: Learn to love tab-completion! Typing the first one or two letters of a function or variable you've used previously then hitting tab will auto-complete that command, saving you typing (i.e. time and mistakes!). \n",
    "\n",
    "Previously, we calculated the lexical diversity of a text. In NLTK, we can create a function called **`lexical_diversity()`** that runs a single line of code. We can then call this function to quickly determine the lexical density of a corpus or subcorpus.\n",
    "\n",
    "**Challenge!**\n",
    "\n",
    "Write a function to calculate the lexical diversity of a text; test it out on the books in the NLTK corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(text):\n",
    "    return len(text)/len(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# After the function has been defined, we can run it:\n",
    "lexical_diversity(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parentheses are important here as they sepatate the the task, that is the work of the function, from the data that the function is to be performed on. \n",
    "\n",
    "The data in parentheses is called the argument of the function. When we use a function, we say that we 'call' it. \n",
    "\n",
    "Other functions that we've used already include **`len()`** and **`sorted()`** - these were predefined. **`lexical_diversity()`** is one we set up ourselves; note that it's conventional to put a set of parentheses after a function, to make it clear what we're talking about.\n",
    "\n",
    "### Lists\n",
    "\n",
    "NLTK treats a text as a long list of words. First, we'll make some lists of our own, to give you an idea of how a list behaves.\n",
    "\n",
    "> **Note**: we use square brackets `[]` here to define our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent0 = ['Call', 'me', 'Ishmael', '.']\n",
    "print(sent0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(sent0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The opening sentences of each of our texts have been pre-defined for you. You can inspect them by typing in 'sent2' etc.\n",
    "\n",
    "You can *concatenate*, or join up lists together, creating a new list containing all the items from both lists. You can do this by typing out the two lists or you can add two or more pre-defined lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sent4)\n",
    "print(sent0)\n",
    "print(sent4 + sent0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add an item to the end of a list by appending. When we **`append()`**, the list itself is updated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent0.append('Please')\n",
    "print(sent0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge!** \n",
    "\n",
    "Define a function called **`please()`** that would append the word 'Please' after a list and return it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def please(sentence):\n",
    "    return sentence.append('please')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Indexing Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can navigate this list with the help of indexes. Just as we can find out the number of times a word occurs in a text, we can also find where a word first occurs. We can navigate to different points in a text without restriction, so long as we can describe where we want to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(text4.index('awaken'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works in reverse as well. We can ask Python to locate the 158th item in our list (note that we use square brackets here, not parentheses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(text4[158])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as pulling out individual items from a list, indexes can be used to pull out selections of text from a large corpus to inspect. We call this slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(text5[16715:16735])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're asking for the beginning or end of a text, we can leave out the first or second number. For instance, [:5] will give us the first five items in a list while [8:] will give us all the elements from the eighth to the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(text2[:10])\n",
    "print(text4[145700:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you understand how indexes work, let's create one.\n",
    "\n",
    "We start by defining the name of our index and then add the items. You probably won't do this in your own work, but you may want to manipulate an index in other ways. Pay attention to the quote marks and commas when you create your test sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent = ['The', 'quick', 'brown', 'fox']\n",
    "print(sent[0])\n",
    "print(sent[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first element in the list is zero. This is because we are telling Python to go zero steps forward in the list. If we use an index that is too large (that is, we ask for something that doesn't exist), we'll get an error.\n",
    "\n",
    "We can modify elements in a list by assigning new data to one of its index values. We can also replace a slice with new material."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent[2] = 'furry'\n",
    "sent[3] = 'child'\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge**\n",
    "\n",
    "- Create a list called 'opening' that consists of the phrase \"It was a dark and stormy night; the rain fell in torrents\"\n",
    "- Create a variable called 'clause' that contains the contents of 'opening', up to the semi-colon\n",
    "- Create a variable called 'alphabetised' that contains the contents of 'clause' sorted alphabetically\n",
    "- Print 'alphabetised' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "opening = ['It', 'was', 'a', 'dark', 'and', 'stormy', 'night', ';', 'the', 'rain', 'fell', 'in', 'torrents']\n",
    "clause = opening[0:7]\n",
    "alphabetised = sorted(clause)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that assigning a variable just causes Python to remember that information without generating any output. \n",
    "\n",
    "If you want Python to show you the result, you have to ask for it (this is a good thing when you assign a variable to a very long list!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(clause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(alphabetised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Python's ability to perform statistical analysis of data to do further exploration of vocabulary. For instance, we might want to be able to find the most common or least common words in a text. We'll start by looking at frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "fdist1 = FreqDist(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist1.most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist1['whale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist1.plot(50, cumulative = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge!**\n",
    "\n",
    "Let's compare the 15 most common tokens of the texts in the NLTK book. You could do this manually, but it will save you time and typing if you define a function and then loop it over the list, 'Library', that you created earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def common_words(text):\n",
    "    return FreqDist(text).most_common(15)\n",
    "common_words(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for book in Library:\n",
    "    words = common_words(book)\n",
    "    print(book, words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Vocab continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as counting individual words, we can count other features of vocabulary, such as how often words of different lengths occur. We do this by putting together a number of the commands we've already learned.\n",
    "\n",
    "We could start like this: \n",
    "\n",
    "     [len(word) for word in text1]\n",
    "\n",
    "... but this would print the length of every word in the whole book, so let's skip that bit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist2 = FreqDist(len(word) for word in text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist2.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist2.freq(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These last two commands tell us that the most common word length is 3, and that these 3 letter words account for about 20% of the book. \n",
    "We can see this just by visually inspecting the list produced by *fdist2.most_common()*, but if this list were too long to inspect readily, or we didn't want to print it, there are other ways to explore it.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of functions defined for NLTK's frequency distributions:\n",
    "\n",
    " | Function | Purpose  |\n",
    " |--------------|------------|\n",
    " | fdist = FreqDist(samples) | create a frequency distribution containing the given samples |\n",
    " | fdist[sample] += 1 | increment the count for this sample |\n",
    " | fdist['monstrous']  | count of the number of times a given sample occurred |\n",
    " | fdist.freq('monstrous') | frequency of a given sample |\n",
    " | fdist.N()  |  total number of samples |\n",
    " | fdist.most_common(n)   |  the n most common samples and their frequencies |\n",
    " | for sample in fdist:   |  iterate over the items in fdist, when in the loop, we refer to each item as sample |\n",
    " | fdist.max() | sample with the greatest count |\n",
    " | fdist.tabulate()   |  tabulate the frequency distribution |\n",
    " | fdist.plot()  |   graphical plot of the frequency distribution |\n",
    " | fdist.plot(cumulative=True) | cumulative plot of the frequency distribution |\n",
    " | fdist1 < fdist2 | test if samples in fdist1 occur less frequently than in fdist2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to select the longest words in a text, which may tell you something about its vocabulary and style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = set(text4)\n",
    "long_words = [word for word in vocab if len(word) > 15]\n",
    "sorted(long_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use numerical operators to refine the types of searches we ask Python to run. We can use the following relational operators:\n",
    "\n",
    "\n",
    "### Common relationals\n",
    " |  Relational | Meaning |\n",
    " |--------------:|:------------|\n",
    " | <    |  less than |\n",
    " | <=   |   less than or equal to |\n",
    " | ==  |    equal to (note this is two \"=\" signs, not one) |\n",
    " | !=   |   not equal to |\n",
    " | \\>   |   greater than |\n",
    " | \\>= |   greater than or equal to |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Challenge!**\n",
    "\n",
    "Using one of the pre-defined sentences in the NLTK corpus, use the relational operators above to find:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Words longer than four characters\n",
    "- Words of four or more characters\n",
    "- Words of exactly four characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "longer = [word for word in sent2 if len(word) > 4]\n",
    "more = [word for word in sent2 if len(word) >= 4]\n",
    "exact = [word for word in sent2 if len(word) == 4]\n",
    "print(longer, more, exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in sent2:\n",
    "    if len(word) > 4:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fine-tune our selection even further by adding other conditions. For instance, we might want to find long words that occur frequently (or rarely).  \n",
    "\n",
    "**Challenge!**\n",
    "\n",
    "Can you find all the words in a text that are more than seven letters long and occur more than seven times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fdist5 = FreqDist(text5)\n",
    "sorted(word for word in set(text5) if len(word) > 7 and fdist5[word] > 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common operators\n",
    "\n",
    " | Operator  | Purpose  |\n",
    " |--------------|------------|\n",
    " | s.startswith(t) | test if s starts with t |\n",
    " | s.endswith(t)  |  test if s ends with t | \n",
    " | t in s         |  test if t is a substring of s | \n",
    " | s.islower()    |  test if s contains cased characters and all are lowercase | \n",
    " | s.isupper()    |  test if s contains cased characters and all are uppercase | \n",
    " | s.isalpha()    |  test if s is non-empty and all characters in s are alphabetic | \n",
    " | s.isalnum()    |  test if s is non-empty and all characters in s are alphanumeric | \n",
    " | s.isdigit()    |  test if s is non-empty and all characters in s are digits | \n",
    " | s.istitle()    |  test if s contains cased characters and is titlecased (i.e. all words in s have initial capitals) | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(w for w in set(text1) if w.endswith('ableness'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted(n for n in sent7 if n.isdigit())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus!**\n",
    "\n",
    "You'll remember right at the beginning we started looking at the size of the vocabulary of a text, but there were two problems with the results we got from using:\n",
    "\n",
    "     len(set(text1)).\n",
    "\n",
    "This count includes items of punctuation and treats capitalised and non-capitalised words as different things (*This* vs *this*). We can now fix these problems. We start by getting rid of capitalised words, then we get rid of the punctuation and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(word.lower() for word in text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(word.lower() for word in text1 if word.isalpha()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:left\" src=\"http://images.catsolonline.com/cache/custom/CEN/CE651-250x250.jpg\" />\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
